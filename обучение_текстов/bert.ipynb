{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad2f0e2",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26296122",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "Необходимо обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Необходимо построить модель со значением метрики качества F1 не меньше 0.75.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Столбец *text* содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4acfe5",
   "metadata": {},
   "source": [
    "## Загрузка и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa32def",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (61.2.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ksiy\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use\n",
      "the full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q spacy\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b8cbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ksiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ksiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings  # Импортируем все необходимые библиотеки для работы\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "from catboost import CatBoostClassifier\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import notebook\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946fccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "try:  \n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "    \n",
    "except:\n",
    "     df = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408276dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # Вывод первых 5 строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9e1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()  # Вывод общей информации на экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332a3a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()  # Соотношение классов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70246a9",
   "metadata": {},
   "source": [
    "**Вывод** Имеем дисбаланс классов примерно 9 к 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dd29a",
   "metadata": {},
   "source": [
    "В текстах необходимо оставить только латинские символы и пробелы. Воспользуемся функцией clear_text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fa21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text2=re.sub(r'[^a-zA-Z ]',' ', text)\n",
    "    return(\" \".join(text2.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7164d5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.73 s\n",
      "Wall time: 2.72 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clear</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>and i really don t think you understand i came...</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    clear  \\\n",
       "0       explanation why the edits made under my userna...   \n",
       "1       d aww he matches this background colour i m se...   \n",
       "2       hey man i m really not trying to edit war it s...   \n",
       "3       more i can t make any real suggestions on impr...   \n",
       "4       you sir are my hero any chance you remember wh...   \n",
       "...                                                   ...   \n",
       "159287  and for the second time of asking when your vi...   \n",
       "159288  you should be ashamed of yourself that is a ho...   \n",
       "159289  spitzer umm theres no actual article for prost...   \n",
       "159290  and it looks like it was actually you who put ...   \n",
       "159291  and i really don t think you understand i came...   \n",
       "\n",
       "                                                     text  \n",
       "0       Explanation\\nWhy the edits made under my usern...  \n",
       "1       D'aww! He matches this background colour I'm s...  \n",
       "2       Hey man, I'm really not trying to edit war. It...  \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...  \n",
       "4       You, sir, are my hero. Any chance you remember...  \n",
       "...                                                   ...  \n",
       "159287  \":::::And for the second time of asking, when ...  \n",
       "159288  You should be ashamed of yourself \\n\\nThat is ...  \n",
       "159289  Spitzer \\n\\nUmm, theres no actual article for ...  \n",
       "159290  And it looks like it was actually you who put ...  \n",
       "159291  \"\\nAnd ... I really don't think you understand...  \n",
       "\n",
       "[159292 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df['clear'] = df['text'].apply(clear_text)\n",
    "df['clear'] = df['clear'].str.lower()\n",
    "df[['clear','text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adbd351",
   "metadata": {},
   "source": [
    "Для лемматизации всех текстов воспользуемся spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb13777a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the stripe bat be hang on their foot for good'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "doc = nlp(sentence)\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38a5216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17min 22s\n",
      "Wall time: 17min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['lemm_text'] = df['clear'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d269296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Первоначальный текст*: 0    explanation why the edits made under my userna...\n",
      "1    d aww he matches this background colour i m se...\n",
      "2    hey man i m really not trying to edit war it s...\n",
      "Name: clear, dtype: object\n",
      "*Преобразованный текст*: 0    explanation why the edit make under my usernam...\n",
      "1    d aww he match this background colour I m seem...\n",
      "2    hey man I m really not try to edit war it s ju...\n",
      "Name: lemm_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"*Первоначальный текст*:\", df['clear'][:3])\n",
    "print(\"*Преобразованный текст*:\", df['lemm_text'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7afe812",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['lemm_text']  # Выделим признаки\n",
    "target = df['toxic']  # Целевой признак\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=123\n",
    ")\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train, target_train, test_size=0.25, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47f2d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ksiy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609339f0",
   "metadata": {},
   "source": [
    "Вычислим TF-IDF для корпуса текстов:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b10088",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdefb1d",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c47d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(\n",
    "    [\n",
    "        (\"vect\", TfidfVectorizer(stop_words=stopwords)),\n",
    "        (\"lr\", LogisticRegression(random_state=123)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a94da",
   "metadata": {},
   "source": [
    "Ввиду сильного дисбаланса классов улучшать метрику F1 будем перевобором порога классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "310bdd28",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Порог = 0.40 | Точность = 0.896, Полнота = 0.664, F1 = 0.763\n",
      "Порог = 0.42 | Точность = 0.906, Полнота = 0.650, F1 = 0.757\n",
      "Порог = 0.44 | Точность = 0.915, Полнота = 0.637, F1 = 0.751\n",
      "Порог = 0.46 | Точность = 0.921, Полнота = 0.627, F1 = 0.746\n",
      "Порог = 0.48 | Точность = 0.927, Полнота = 0.617, F1 = 0.741\n",
      "Порог = 0.50 | Точность = 0.934, Полнота = 0.606, F1 = 0.735\n",
      "Порог = 0.52 | Точность = 0.939, Полнота = 0.593, F1 = 0.727\n",
      "Порог = 0.54 | Точность = 0.944, Полнота = 0.581, F1 = 0.719\n",
      "Порог = 0.56 | Точность = 0.948, Полнота = 0.568, F1 = 0.711\n",
      "Порог = 0.58 | Точность = 0.949, Полнота = 0.559, F1 = 0.703\n",
      "Порог = 0.60 | Точность = 0.953, Полнота = 0.551, F1 = 0.698\n",
      "Порог = 0.62 | Точность = 0.954, Полнота = 0.540, F1 = 0.690\n",
      "Порог = 0.64 | Точность = 0.956, Полнота = 0.532, F1 = 0.684\n",
      "Порог = 0.66 | Точность = 0.960, Полнота = 0.522, F1 = 0.676\n",
      "Порог = 0.68 | Точность = 0.961, Полнота = 0.511, F1 = 0.667\n",
      "Порог = 0.70 | Точность = 0.962, Полнота = 0.502, F1 = 0.660\n",
      "Порог = 0.72 | Точность = 0.965, Полнота = 0.491, F1 = 0.651\n",
      "Порог = 0.74 | Точность = 0.970, Полнота = 0.482, F1 = 0.644\n",
      "Порог = 0.76 | Точность = 0.974, Полнота = 0.469, F1 = 0.633\n",
      "Порог = 0.78 | Точность = 0.977, Полнота = 0.453, F1 = 0.619\n",
      "Порог = 0.80 | Точность = 0.980, Полнота = 0.441, F1 = 0.609\n",
      "Порог = 0.82 | Точность = 0.983, Полнота = 0.429, F1 = 0.598\n",
      "Порог = 0.84 | Точность = 0.988, Полнота = 0.416, F1 = 0.586\n",
      "Порог = 0.86 | Точность = 0.988, Полнота = 0.399, F1 = 0.568\n",
      "Порог = 0.88 | Точность = 0.990, Полнота = 0.379, F1 = 0.548\n"
     ]
    }
   ],
   "source": [
    "pipeline_lr.fit(features_train, target_train)\n",
    "probabilities = pipeline_lr.predict_proba(features_valid)\n",
    "probabilities_one = probabilities[:, 1]\n",
    "\n",
    "for threshold in np.arange(0.4, 0.9, 0.02):\n",
    "    predicted_v = probabilities_one > threshold\n",
    "    precision = precision_score(target_valid, predicted_v)\n",
    "    recall = recall_score(target_valid, predicted_v) \n",
    "    f1 = f1_score(target_valid, predicted_v)\n",
    "    print(\"Порог = {:.2f} | Точность = {:.3f}, Полнота = {:.3f}, F1 = {:.3f}\".format(\n",
    "        threshold, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1deef4",
   "metadata": {},
   "source": [
    "Исходя из полученных данных, примем порог классификации 0.40:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f635d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика F1 качества предсказания модели LogisticRegression с порогом классификации 0,40 равна :  0.7625\n"
     ]
    }
   ],
   "source": [
    "predicted_valid = (pipeline_lr.predict_proba(features_valid)[:,1] > 0.40)*1\n",
    "f1_log_r_1 = f1_score(target_valid, predicted_valid).round(4)\n",
    "print('Метрика F1 качества предсказания модели LogisticRegression с порогом классификации 0,40 равна : ',f1_log_r_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc114df5",
   "metadata": {},
   "source": [
    "### svm. LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0977b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svc = Pipeline(\n",
    "    [\n",
    "        (\"vect\", TfidfVectorizer(stop_words=stopwords)),\n",
    "        (\"lsvc\", svm.LinearSVC(random_state=123)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc782835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика F1 качества предсказания модели LinearSVC равна :  0.7751\n",
      "CPU times: total: 21.9 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_svc.fit(features_train, target_train)\n",
    "score_svc = cross_val_score(pipeline_svc, features_train, target_train, cv=3, scoring='f1').mean()\n",
    "print('Метрика F1 качества предсказания модели LinearSVC равна : ',score_svc.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7213a3",
   "metadata": {},
   "source": [
    "**Вывод** Обе модели показали метрики, соответствующие заданию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a46dc",
   "metadata": {},
   "source": [
    "## Обучение с BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bde169e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert = df.sample(1000)  # Для обучения модели возьмем 3000 строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da1ea23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    910\n",
       "1     90\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert['toxic'].value_counts()  # Проверяем баланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6de6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bert = df_bert['text']  # Выделяем признаки\n",
    "target_bert = df_bert['toxic']  # Целевой признак\n",
    "features_train_bert, features_test_bert, target_train_bert, target_test_bert = train_test_split(\n",
    "    features_bert, target_bert, stratify=df_bert['toxic'], test_size=0.5, random_state=1234\n",
    ")  # Размер тренировочной и тестовой выборок возьмем 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57cbdbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at unitary/toxic-bert were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertModel.from_pretrained(\n",
    "    \"unitary/toxic-bert\")  # Инициализируем модель Bert\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    \"unitary/toxic-bert\")\n",
    "\n",
    "\n",
    "def tokenize(features): # Функция разделения текста на токены и кодирования\n",
    "\n",
    "    tokenized = features.apply(\n",
    "        lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True))\n",
    "\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "121667f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(padded):  # Функция перевода предобработанных токенов в векторы\n",
    "    \n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    batch_size = 100\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(padded.shape[0] // batch_size)):\n",
    "            batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "            attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "            embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d41e4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = tokenize(features_train_bert)\n",
    "padded_test = tokenize(features_test_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6432ae14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21a4fdf902a4591b286368d747036ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5835cdfe57843529ec0d7f057c1767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_train = embeddings(padded_train)\n",
    "embeddings_test = embeddings(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96830650",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train2 = np.concatenate(embeddings_train)\n",
    "features_test2 = np.concatenate(embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464530d",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ef3aa",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a74da",
   "metadata": {},
   "source": [
    "В этот раз попробуем учесть дисбаланс классов с помощью параметра class_weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6a24474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика на тренировочной выборке F1  = 0.9146\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced', random_state=1234)\n",
    "\n",
    "score = cross_val_score(model, features_train2, target_train_bert, cv=3, scoring='f1').mean()\n",
    "print(\"Метрика на тренировочной выборке F1  = %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9a9e7",
   "metadata": {},
   "source": [
    "### CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eedfc3c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.006442\n",
      "0:\tlearn: 0.6792423\ttotal: 148ms\tremaining: 2m 28s\n",
      "100:\tlearn: 0.1242228\ttotal: 7.98s\tremaining: 1m 11s\n",
      "200:\tlearn: 0.0394755\ttotal: 15.8s\tremaining: 1m 2s\n",
      "300:\tlearn: 0.0186161\ttotal: 23.6s\tremaining: 54.7s\n",
      "400:\tlearn: 0.0110520\ttotal: 31.3s\tremaining: 46.8s\n",
      "500:\tlearn: 0.0075789\ttotal: 39.1s\tremaining: 38.9s\n",
      "600:\tlearn: 0.0056321\ttotal: 46.9s\tremaining: 31.1s\n",
      "700:\tlearn: 0.0044147\ttotal: 54.6s\tremaining: 23.3s\n",
      "800:\tlearn: 0.0036110\ttotal: 1m 2s\tremaining: 15.5s\n",
      "900:\tlearn: 0.0030328\ttotal: 1m 10s\tremaining: 7.71s\n",
      "999:\tlearn: 0.0026091\ttotal: 1m 17s\tremaining: 0us\n",
      "Learning rate set to 0.006442\n",
      "0:\tlearn: 0.6795537\ttotal: 85.5ms\tremaining: 1m 25s\n",
      "100:\tlearn: 0.1312475\ttotal: 7.92s\tremaining: 1m 10s\n",
      "200:\tlearn: 0.0414920\ttotal: 16.1s\tremaining: 1m 4s\n",
      "300:\tlearn: 0.0198466\ttotal: 24.2s\tremaining: 56.1s\n",
      "400:\tlearn: 0.0119154\ttotal: 32.1s\tremaining: 47.9s\n",
      "500:\tlearn: 0.0081035\ttotal: 40.2s\tremaining: 40.1s\n",
      "600:\tlearn: 0.0060010\ttotal: 48.3s\tremaining: 32.1s\n",
      "700:\tlearn: 0.0047086\ttotal: 56.5s\tremaining: 24.1s\n",
      "800:\tlearn: 0.0038717\ttotal: 1m 4s\tremaining: 16.1s\n",
      "900:\tlearn: 0.0032519\ttotal: 1m 12s\tremaining: 8s\n",
      "999:\tlearn: 0.0027900\ttotal: 1m 20s\tremaining: 0us\n",
      "Learning rate set to 0.00645\n",
      "0:\tlearn: 0.6793676\ttotal: 85.3ms\tremaining: 1m 25s\n",
      "100:\tlearn: 0.1286032\ttotal: 7.9s\tremaining: 1m 10s\n",
      "200:\tlearn: 0.0404559\ttotal: 15.7s\tremaining: 1m 2s\n",
      "300:\tlearn: 0.0187589\ttotal: 23.5s\tremaining: 54.7s\n",
      "400:\tlearn: 0.0111201\ttotal: 31.3s\tremaining: 46.8s\n",
      "500:\tlearn: 0.0076103\ttotal: 39.1s\tremaining: 38.9s\n",
      "600:\tlearn: 0.0056752\ttotal: 46.9s\tremaining: 31.1s\n",
      "700:\tlearn: 0.0044740\ttotal: 54.6s\tremaining: 23.3s\n",
      "800:\tlearn: 0.0036736\ttotal: 1m 2s\tremaining: 15.5s\n",
      "900:\tlearn: 0.0031043\ttotal: 1m 10s\tremaining: 7.71s\n",
      "999:\tlearn: 0.0026832\ttotal: 1m 18s\tremaining: 0us\n",
      "CPU times: total: 29min 54s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_c = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    verbose=100,\n",
    "    random_state=1234\n",
    ") \n",
    "score_c = cross_val_score(model_c, features_train2, target_train_bert, cv=3, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e39f2957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика на тренировочной выборке F1 = 0.8419\n"
     ]
    }
   ],
   "source": [
    "print(\"Метрика на тренировочной выборке F1 = %.4f\" % score_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ee924",
   "metadata": {},
   "source": [
    "**Вывод**  За лучшую модель примем модель логистической регрессии, обученной на данных, преобразованных с помощью BERT, c метрикой F1 = 0.9146"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c704ccb",
   "metadata": {},
   "source": [
    "## Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e894fb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестовая метрика F1 финальной модели:  0.93\n"
     ]
    }
   ],
   "source": [
    "model.fit(features_train2, target_train_bert)\n",
    "pred = model.predict(features_test2)\n",
    "print(\"Тестовая метрика F1 финальной модели: \", f1_score(target_test_bert, pred).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcfde91",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa0a02",
   "metadata": {},
   "source": [
    "В результате работы был проведен анализ входных данных, выявлен дисбаланс классов. Задача решалась двумя способами: обработкой и лемматизацией текстов с помощью spaCy и предобработкой текстов с помощью библиотеки transformers и BERT-модели. В каждом из вариантов данные были разделены на тренировочную и тестовую выборки, затем обучены модели. Дисбаланс классов учитывался в модели логистической регрессии с помощью подбора порога классификации и указанием параметра class_weight='balanced'.\\\n",
    "Наилучшую метрику на валидационной выборке показала модель LogisticRegression с предобработкой текстов с помощью BERT: F1 = 0.9146.\\\n",
    "Затем, модель была протестирована на тестовой выборке.\\\n",
    "**На тестовой выборке модель LogisticRegression показала результат F1 = 0.93**, что соответствует заданию задачи."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 9429,
    "start_time": "2023-03-05T18:09:31.101Z"
   },
   {
    "duration": 2091,
    "start_time": "2023-03-05T18:09:40.533Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-05T18:09:42.626Z"
   },
   {
    "duration": 30,
    "start_time": "2023-03-05T18:09:42.641Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-05T18:09:42.673Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-05T18:09:42.680Z"
   },
   {
    "duration": 13,
    "start_time": "2023-03-05T18:09:42.685Z"
   },
   {
    "duration": 872,
    "start_time": "2023-03-05T18:09:42.714Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-05T18:09:43.588Z"
   },
   {
    "duration": 8718,
    "start_time": "2023-03-05T19:28:04.384Z"
   },
   {
    "duration": 2499,
    "start_time": "2023-03-05T19:28:13.105Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-05T19:28:15.605Z"
   },
   {
    "duration": 42,
    "start_time": "2023-03-05T19:28:15.623Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-05T19:28:15.667Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-05T19:28:15.674Z"
   },
   {
    "duration": 2467,
    "start_time": "2023-03-05T19:28:15.680Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-05T19:28:18.150Z"
   },
   {
    "duration": 378,
    "start_time": "2023-03-05T19:28:18.166Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-05T19:28:18.549Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-05T19:28:57.541Z"
   },
   {
    "duration": 46,
    "start_time": "2023-03-05T19:28:58.491Z"
   },
   {
    "duration": 785,
    "start_time": "2023-03-05T19:29:57.747Z"
   },
   {
    "duration": 1748,
    "start_time": "2023-03-05T19:30:10.868Z"
   },
   {
    "duration": 7197,
    "start_time": "2023-03-05T19:30:26.996Z"
   },
   {
    "duration": 45,
    "start_time": "2023-03-05T19:32:04.169Z"
   },
   {
    "duration": 60,
    "start_time": "2023-03-05T19:39:52.502Z"
   },
   {
    "duration": 4130583,
    "start_time": "2023-03-05T19:40:19.214Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-05T20:49:09.798Z"
   },
   {
    "duration": 587,
    "start_time": "2023-03-05T20:56:31.703Z"
   },
   {
    "duration": 949,
    "start_time": "2023-03-05T20:56:32.298Z"
   },
   {
    "duration": 24,
    "start_time": "2023-03-05T20:56:33.249Z"
   },
   {
    "duration": 46,
    "start_time": "2023-03-05T20:56:33.275Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-05T20:56:33.323Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-05T20:56:33.332Z"
   },
   {
    "duration": 2858,
    "start_time": "2023-03-05T20:56:33.337Z"
   },
   {
    "duration": 5,
    "start_time": "2023-03-05T20:56:36.197Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-05T20:56:36.204Z"
   },
   {
    "duration": 279,
    "start_time": "2023-03-05T20:56:36.222Z"
   },
   {
    "duration": 4865,
    "start_time": "2023-03-05T20:58:08.295Z"
   },
   {
    "duration": 973,
    "start_time": "2023-03-05T20:58:13.165Z"
   },
   {
    "duration": 22,
    "start_time": "2023-03-05T20:58:14.140Z"
   },
   {
    "duration": 62,
    "start_time": "2023-03-05T20:58:14.165Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-05T20:58:14.228Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-05T20:58:14.235Z"
   },
   {
    "duration": 2800,
    "start_time": "2023-03-05T20:58:14.240Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-05T20:58:17.042Z"
   },
   {
    "duration": 1960,
    "start_time": "2023-03-05T20:58:17.057Z"
   },
   {
    "duration": 498,
    "start_time": "2023-03-05T20:58:19.020Z"
   },
   {
    "duration": 153,
    "start_time": "2023-03-05T20:58:34.969Z"
   },
   {
    "duration": 266,
    "start_time": "2023-03-05T20:59:03.379Z"
   },
   {
    "duration": 82,
    "start_time": "2023-03-05T21:04:19.898Z"
   },
   {
    "duration": 87,
    "start_time": "2023-03-05T21:04:34.937Z"
   },
   {
    "duration": 318,
    "start_time": "2023-03-05T21:05:06.724Z"
   },
   {
    "duration": 2731,
    "start_time": "2023-03-05T21:06:59.079Z"
   },
   {
    "duration": 71,
    "start_time": "2023-03-05T21:07:42.338Z"
   },
   {
    "duration": 1408094,
    "start_time": "2023-03-05T21:07:52.833Z"
   },
   {
    "duration": 21,
    "start_time": "2023-03-05T21:45:19.871Z"
   },
   {
    "duration": 4,
    "start_time": "2023-03-05T21:46:10.007Z"
   },
   {
    "duration": 14,
    "start_time": "2023-03-05T21:49:11.564Z"
   },
   {
    "duration": 1910,
    "start_time": "2023-03-05T21:49:52.685Z"
   },
   {
    "duration": 5458,
    "start_time": "2023-03-05T21:51:11.546Z"
   },
   {
    "duration": 1007,
    "start_time": "2023-03-05T21:51:17.006Z"
   },
   {
    "duration": 18,
    "start_time": "2023-03-05T21:51:18.015Z"
   },
   {
    "duration": 52,
    "start_time": "2023-03-05T21:51:18.036Z"
   },
   {
    "duration": 6,
    "start_time": "2023-03-05T21:51:18.089Z"
   },
   {
    "duration": 3,
    "start_time": "2023-03-05T21:51:18.097Z"
   },
   {
    "duration": 515,
    "start_time": "2023-03-05T21:51:21.100Z"
   },
   {
    "duration": 147,
    "start_time": "2023-03-05T21:51:28.930Z"
   },
   {
    "duration": 2493,
    "start_time": "2023-03-05T21:51:38.498Z"
   },
   {
    "duration": 82,
    "start_time": "2023-03-05T21:52:22.563Z"
   },
   {
    "duration": 807,
    "start_time": "2023-03-05T21:53:21.386Z"
   },
   {
    "duration": 5357,
    "start_time": "2023-03-05T21:59:56.247Z"
   },
   {
    "duration": 882,
    "start_time": "2023-03-05T22:00:01.606Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-05T22:00:02.490Z"
   },
   {
    "duration": 45,
    "start_time": "2023-03-05T22:00:02.508Z"
   },
   {
    "duration": 16,
    "start_time": "2023-03-05T22:00:02.555Z"
   },
   {
    "duration": 28,
    "start_time": "2023-03-05T22:00:02.572Z"
   },
   {
    "duration": 2671,
    "start_time": "2023-03-05T22:00:02.602Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
